{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "###############################################################################################\n",
    "    10-Armed Testbed (Reinforcement Learning: An Introduction, Sutton, Barto, fig 2.2)\n",
    "    Created by Youn-Hee Han 12/27/2019, last update 12/27/2019\n",
    "###############################################################################################\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 상태와 행동 및 문제에 대한 설정 규칙을 포함하는 테스트베드 클래스\n",
    "class Testbed(object):\n",
    "    # Constructor\n",
    "    def __init__(self, num_arms, mean, std):\n",
    "        # Number of arms\n",
    "        self.num_arms = num_arms\n",
    "\n",
    "        # 참 가치와 보상 값 설정을 위한 정규 분포 파라미터\n",
    "        self.mean = mean        # 평균\n",
    "        self.std = std        # 표준 편차\n",
    "\n",
    "        self.acts = np.zeros(num_arms)   # Array to store action values\n",
    "        self.optimal = 0                  # Store optimal value for greedy\n",
    "        self.reset()\n",
    "\n",
    "    # Reset testbed for next iteration\n",
    "    def reset(self):\n",
    "        # Set random gaussian/normal values using numpy function, requires mean, standard deviation and number of arms\n",
    "        self.acts = np.random.normal(self.mean, self.std, self.num_arms)\n",
    "\n",
    "        # Identify the maximum value in action array\n",
    "        self.optimal = np.argmax(self.actArr)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Agent Class - Controls the agents movement and behaviour in the environment interacting with the testbed\n",
    "# and receives information on the current position\n",
    "class Agent(object):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self,nArms, eProb=0):\n",
    "        self.nArms = nArms      # Number of arms\n",
    "        self.eProb = eProb      # Epsilon probability\n",
    "\n",
    "        self.timeStep = 0                    # Time Step t\n",
    "        self.lastAction = None               # Store last action\n",
    "\n",
    "        self.kAction = np.zeros(nArms)          # count of actions taken at time t\n",
    "        self.rSum = np.zeros(nArms)             # Sums number of rewards\n",
    "        self.valEstimates = np.zeros(nArms)     # action value estimates sum(rewards)/Amount\n",
    "\n",
    "\n",
    "    # Return string for graph legend\n",
    "    def __str__(self):\n",
    "        if self.eProb == 0:\n",
    "            return \"Greedy\"\n",
    "        else:\n",
    "            return \"Epsilon = \" + str(self.eProb)\n",
    "\n",
    "\n",
    "    # Selects action based on a epsilon-greedy behaviour,\n",
    "    # if epsilon equals zero, then the agent performs a greedy selection\n",
    "    def action(self):\n",
    "\n",
    "        ### POLICY ###\n",
    "        # Epsilon method\n",
    "        randProb = np.random.random()   # Pick random probability between 0-1\n",
    "        if randProb < self.eProb:\n",
    "            a = np.random.choice(len(self.valEstimates))    # Select random action\n",
    "\n",
    "        # Greedy Method\n",
    "        else:\n",
    "            maxAction = np.argmax(self.valEstimates)     # Find max value estimate\n",
    "            # identify the corresponding action, as array containing only actions with max\n",
    "            action = np.where(self.valEstimates == np.argmax(self.valEstimates))[0]\n",
    "\n",
    "            # If multiple actions contain the same value, randomly select an action\n",
    "            if len(action) == 0:\n",
    "                a = maxAction\n",
    "            else:\n",
    "                a = np.random.choice(action)\n",
    "\n",
    "        # save last action in variable, and return result\n",
    "        self.lastAction = a\n",
    "        return a\n",
    "\n",
    "\n",
    "    # Interpreter - updates the value extimates amounts based on the last action\n",
    "    def interpreter(self, reward):\n",
    "        # Add 1 to the number of action taken in step\n",
    "        At = self.lastAction\n",
    "\n",
    "        self.kAction[At] += 1       # Add 1 to action selection\n",
    "        self.rSum[At] += reward     # Add reward to sum array\n",
    "\n",
    "        # Calculate new action-value, sum(r)/ka\n",
    "        self.valEstimates[At] = self.rSum[At]/self.kAction[At]\n",
    "\n",
    "        # Increase time step\n",
    "        self.timeStep += 1\n",
    "\n",
    "\n",
    "    # Reset all variables for next iteration\n",
    "    def reset(self):\n",
    "        self.timeStep = 0                    # Time Step t\n",
    "        self.lastAction = None               # Store last action\n",
    "\n",
    "        self.kAction[:] = 0                  # count of actions taken at time t\n",
    "        self.rSum[:] = 0\n",
    "        self.valEstimates[:] = 0   # action value estimates Qt ~= Q*(a)\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Environment class to control all objects (agent/Testbed)\n",
    "class Environment(object):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, testbed, agents, plays, iterations):\n",
    "        self.testbed = testbed\n",
    "        self.agents = agents\n",
    "\n",
    "        self.plays = plays\n",
    "        self.iterations = iterations\n",
    "\n",
    "\n",
    "    # Run Test\n",
    "    def play(self):\n",
    "\n",
    "        # Array to store the scores, number of plays X number of agents\n",
    "        scoreArr = np.zeros((self.plays, len(self.agents)))\n",
    "        # Array to maintain optimal count, Graph 2\n",
    "        optimlArr = np.zeros((self.plays, len(self.agents)))\n",
    "\n",
    "        # loop for number of iterations\n",
    "        for iIter in range(self.iterations):\n",
    "\n",
    "            # Print statement after every 100 iterations\n",
    "            if (iIter%100) == 0:\n",
    "                print(\"Completed Iterations: \",iIter)\n",
    "\n",
    "            #Reset testbed and all agents\n",
    "            self.testbed.reset()\n",
    "            for agent in self.agents:\n",
    "                agent.reset()\n",
    "\n",
    "\n",
    "            # Loop for number of plays\n",
    "            for jPlays in range(self.plays):\n",
    "                agtCnt = 0\n",
    "\n",
    "                for kAgent in self.agents:\n",
    "                    actionT =  kAgent.action()\n",
    "\n",
    "                    # Reward - normal dist (Q*(at), variance = 1)\n",
    "                    rewardT = np.random.normal(self.testbed.actArr[actionT], scale=1)\n",
    "\n",
    "                    # Agent checks state\n",
    "                    kAgent.interpreter(reward=rewardT)\n",
    "\n",
    "                    # Add score in arrary, graph 1\n",
    "                    scoreArr[jPlays,agtCnt] += rewardT\n",
    "\n",
    "                    # check the optimal action, add optimal to array, graph 2\n",
    "                    if actionT == self.testbed.optim:\n",
    "                        optimlArr[jPlays,agtCnt] += 1\n",
    "\n",
    "                    agtCnt += 1\n",
    "\n",
    "        #return averages\n",
    "        scoreAvg = scoreArr/self.iterations\n",
    "        optimlAvg = optimlArr/self.iterations\n",
    "\n",
    "        return scoreAvg, optimlAvg\n",
    "\n",
    "\n",
    "################################################################\n",
    "## MAIN ##\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()    #store time to monitor execution\n",
    "    nArms = 10                  # n number of bandits\n",
    "    iterations = 2000         # number of repeated iterations\n",
    "    plays = 1000                # number of pplays per iteration\n",
    "\n",
    "    # Setup objects to contain infomration about the agents, testbed, and environment\n",
    "    testbed = Testbed(nArms=nArms,mean=0,stDev=1)\n",
    "    agents = [Agent(nArms=nArms),Agent(nArms=nArms,eProb=0.1),Agent(nArms=nArms,eProb=0.01)]\n",
    "    environment = Environment(testbed=testbed,agents=agents,plays=plays,iterations=iterations)\n",
    "\n",
    "    # Run Environment\n",
    "    print(\"Running...\")\n",
    "    g1Scores, g2Optimal = environment.play()\n",
    "    print(\"Execution time: %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    #Graph 1 - Averate rewards over all plays\n",
    "    plt.title(\"10-Armed TestBed - Average Rewards\")\n",
    "    plt.plot(g1Scores)\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.xlabel('Plays')\n",
    "    plt.legend(agents, loc=4)\n",
    "    plt.show()\n",
    "\n",
    "    #Graph 1 - optimal selections over all plays\n",
    "    plt.title(\"10-Armed TestBed - % Optimal Action\")\n",
    "    plt.plot(g2Optimal * 100)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.ylabel('% Optimal Action')\n",
    "    plt.xlabel('Plays')\n",
    "    plt.legend(agents, loc=4)\n",
    "    plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}