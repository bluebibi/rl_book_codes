{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 DQN 甑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-01-07 00:41:06,337] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode:     1] Reward:  27.0 -greedy:  1.00\n",
      "[Episode:     2] Reward:  12.0 -greedy:  0.98\n",
      "[Episode:     3] Reward:  17.0 -greedy:  0.96\n",
      "[Episode:     4] Reward:  32.0 -greedy:  0.94\n",
      "[Episode:     5] Reward:  38.0 -greedy:  0.92\n",
      "[Episode:     6] Reward:  40.0 -greedy:  0.90\n",
      "[Episode:     7] Reward:  22.0 -greedy:  0.88\n",
      "[Episode:     8] Reward:  11.0 -greedy:  0.86\n",
      "[Episode:     9] Reward:  15.0 -greedy:  0.84\n",
      "[Episode:    10] Reward:  15.0 -greedy:  0.82\n",
      "[Episode:    11] Reward:  47.0 -greedy:  0.80\n",
      "[Episode:    12] Reward:  12.0 -greedy:  0.78\n",
      "[Episode:    13] Reward:  11.0 -greedy:  0.76\n",
      "[Episode:    14] Reward:  15.0 -greedy:  0.74\n",
      "[Episode:    15] Reward:  28.0 -greedy:  0.72\n",
      "[Episode:    16] Reward:  11.0 -greedy:  0.70\n",
      "[Episode:    17] Reward:  15.0 -greedy:  0.68\n",
      "[Episode:    18] Reward:  29.0 -greedy:  0.66\n",
      "[Episode:    19] Reward:  15.0 -greedy:  0.64\n",
      "[Episode:    20] Reward:  33.0 -greedy:  0.62\n",
      "[Episode:    21] Reward:  21.0 -greedy:  0.60\n",
      "[Episode:    22] Reward:  67.0 -greedy:  0.58\n",
      "[Episode:    23] Reward:   8.0 -greedy:  0.56\n",
      "[Episode:    24] Reward:  14.0 -greedy:  0.54\n",
      "[Episode:    25] Reward:  15.0 -greedy:  0.52\n",
      "[Episode:    26] Reward:  12.0 -greedy:  0.51\n",
      "[Episode:    27] Reward:  31.0 -greedy:  0.49\n",
      "[Episode:    28] Reward:  15.0 -greedy:  0.47\n",
      "[Episode:    29] Reward:  17.0 -greedy:  0.45\n",
      "[Episode:    30] Reward:  11.0 -greedy:  0.43\n",
      "[Episode:    31] Reward:  12.0 -greedy:  0.41\n",
      "[Episode:    32] Reward:  19.0 -greedy:  0.39\n",
      "[Episode:    33] Reward:  20.0 -greedy:  0.37\n",
      "[Episode:    34] Reward:  27.0 -greedy:  0.35\n",
      "[Episode:    35] Reward:  35.0 -greedy:  0.33\n",
      "[Episode:    36] Reward:  20.0 -greedy:  0.31\n",
      "[Episode:    37] Reward:  29.0 -greedy:  0.29\n",
      "[Episode:    38] Reward:  32.0 -greedy:  0.27\n",
      "[Episode:    39] Reward:  41.0 -greedy:  0.25\n",
      "[Episode:    40] Reward:  57.0 -greedy:  0.23\n",
      "[Episode:    41] Reward:  61.0 -greedy:  0.21\n",
      "[Episode:    42] Reward:  21.0 -greedy:  0.19\n",
      "[Episode:    43] Reward: 111.0 -greedy:  0.17\n",
      "[Episode:    44] Reward:  34.0 -greedy:  0.15\n",
      "[Episode:    45] Reward:  84.0 -greedy:  0.13\n",
      "[Episode:    46] Reward:  61.0 -greedy:  0.11\n",
      "[Episode:    47] Reward: 152.0 -greedy:  0.09\n",
      "[Episode:    48] Reward:  78.0 -greedy:  0.07\n",
      "[Episode:    49] Reward:  57.0 -greedy:  0.05\n",
      "[Episode:    50] Reward:  86.0 -greedy:  0.03\n",
      "[Episode:    51] Reward:  70.0 -greedy:  0.01\n",
      "[Episode:    52] Reward: 149.0 -greedy:  0.01\n",
      "[Episode:    53] Reward: 162.0 -greedy:  0.01\n",
      "[Episode:    54] Reward: 167.0 -greedy:  0.01\n",
      "[Episode:    55] Reward: 185.0 -greedy:  0.01\n",
      "[Episode:    56] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    57] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    58] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    59] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    60] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    61] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    62] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    63] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    64] Reward: 200.0 -greedy:  0.01\n",
      "[Episode:    65] Reward: 200.0 -greedy:  0.01\n",
      "10 检 办 标车\n",
      "Mean reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from typing import List, Tuple\n",
    "\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.final = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "Transition = namedtuple(\"Transition\",\n",
    "                        field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.capacity = capacity\n",
    "        self.cursor = 0\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: int,\n",
    "             next_state: np.ndarray,\n",
    "             done: bool) -> None:\n",
    "\n",
    "        if len(self) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.cursor] = Transition(state,\n",
    "                                              action, reward, next_state, done)\n",
    "        self.cursor = (self.cursor + 1) % self.capacity\n",
    "\n",
    "    def pop(self, batch_size: int) -> List[Transition]:\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        self.dqn = DQN(input_dim, output_dim, hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.dqn.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray, eps: float) -> int:\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.choice(self.output_dim)\n",
    "        else:\n",
    "            self.dqn.train(mode=False)\n",
    "            scores = self.get_Q(states)\n",
    "            _, argmax = torch.max(scores.data, 1)\n",
    "            return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        states = self._to_variable(states.reshape(-1, self.input_dim))\n",
    "        self.dqn.train(mode=False)\n",
    "        return self.dqn(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        self.dqn.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def train_helper(agent: Agent, minibatch: List[Transition], gamma: float) -> float:\n",
    "    states = np.vstack([x.state for x in minibatch])\n",
    "    actions = np.array([x.action for x in minibatch])\n",
    "    rewards = np.array([x.reward for x in minibatch])\n",
    "    next_states = np.vstack([x.next_state for x in minibatch])\n",
    "    done = np.array([x.done for x in minibatch])\n",
    "\n",
    "    Q_predict = agent.get_Q(states)\n",
    "    Q_target = Q_predict.clone().data.numpy()\n",
    "    Q_target[np.arange(len(Q_target)), actions] = rewards + gamma * np.max(agent.get_Q(next_states).data.numpy(), axis=1) * ~done\n",
    "    Q_target = agent._to_variable(Q_target)\n",
    "\n",
    "    return agent.train(Q_predict, Q_target)\n",
    "\n",
    "\n",
    "def play_episode(env: gym.Env,\n",
    "                 agent: Agent,\n",
    "                 replay_memory: ReplayMemory,\n",
    "                 eps: float,\n",
    "                 batch_size: int) -> int:\n",
    "\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "        if done:\n",
    "            r = -1\n",
    "        replay_memory.push(s, a, r, s2, done)\n",
    "\n",
    "        if len(replay_memory) > batch_size:\n",
    "\n",
    "            minibatch = replay_memory.pop(batch_size)\n",
    "            train_helper(agent, minibatch, 0.99)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def get_env_dim(env: gym.Env) -> Tuple[int, int]:\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    return input_dim, output_dim\n",
    "\n",
    "\n",
    "def epsilon_annealing(epsiode: int, max_episode: int, min_eps: float) -> float:\n",
    "    slope = (min_eps - 1.0) / max_episode\n",
    "    return max(slope * epsiode + 1.0, min_eps)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main\n",
    "    \"\"\"\n",
    "    try:\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        #env = gym.wrappers.Monitor(env, directory=\"monitors\", force=True)\n",
    "        rewards = deque(maxlen=10)\n",
    "        input_dim, output_dim = get_env_dim(env)\n",
    "        agent = Agent(input_dim, output_dim, 32)\n",
    "        replay_memory = ReplayMemory(50000)\n",
    "\n",
    "        for i in range(200):\n",
    "            eps = epsilon_annealing(i, 50, 0.01)\n",
    "            r = play_episode(env, agent, replay_memory, eps, 64)\n",
    "            print(\"[Episode: {:5}] Reward: {:5} -greedy: {:5.2f}\".format(i + 1, r, eps))\n",
    "\n",
    "            rewards.append(r)\n",
    "\n",
    "            if len(rewards) == rewards.maxlen:\n",
    "\n",
    "                if np.mean(rewards) >= 200:\n",
    "                    print(\"10 检 办 标车\")\n",
    "                    print(\"Mean reward: {}\".format(np.mean(rewards)))\n",
    "                    break\n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
